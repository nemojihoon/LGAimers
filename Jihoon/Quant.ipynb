{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 41840,
     "status": "ok",
     "timestamp": 1770373645102,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "lRVzkcGddch9",
    "outputId": "86aeb506-65ab-4205-8ac6-052dffb11d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmcompressor\n",
      "  Downloading llmcompressor-0.9.0.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
      "Requirement already satisfied: numpy<=2.3.5,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
      "Collecting tqdm<=4.67.1,>=4.66.3 (from llmcompressor)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<=2.9.1,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.9.0+cu128)\n",
      "Collecting transformers<=4.57.3,>=4.54.0 (from llmcompressor)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets<=4.4.1,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.0.0)\n",
      "Requirement already satisfied: accelerate<=1.12.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.12.0)\n",
      "Collecting nvidia-ml-py<=13.590.44,>=12.560.30 (from llmcompressor)\n",
      "  Downloading nvidia_ml_py-13.590.44-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: pillow<=12.0.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
      "Collecting compressed-tensors==0.13.0 (from llmcompressor)\n",
      "  Downloading compressed_tensors-0.13.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.13.0->llmcompressor) (2.12.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (5.9.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2026.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.5.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate<=1.12.0,>=1.6.0->llmcompressor)\n",
      "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.57.3,>=4.54.0->llmcompressor) (0.22.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate<=1.12.0,>=1.6.0->llmcompressor) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.9.1,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.9.1,>=2.7.0->llmcompressor) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.17.0)\n",
      "Downloading llmcompressor-0.9.0.1-py3-none-any.whl (282 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading compressed_tensors-0.13.0-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-13.590.44-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-ml-py, tqdm, loguru, huggingface_hub, transformers, compressed-tensors, llmcompressor\n",
      "  Attempting uninstall: nvidia-ml-py\n",
      "    Found existing installation: nvidia-ml-py 13.590.48\n",
      "    Uninstalling nvidia-ml-py-13.590.48:\n",
      "      Successfully uninstalled nvidia-ml-py-13.590.48\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.3\n",
      "    Uninstalling tqdm-4.67.3:\n",
      "      Successfully uninstalled tqdm-4.67.3\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface_hub 1.4.0\n",
      "    Uninstalling huggingface_hub-1.4.0:\n",
      "      Successfully uninstalled huggingface_hub-1.4.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.0.0\n",
      "    Uninstalling transformers-5.0.0:\n",
      "      Successfully uninstalled transformers-5.0.0\n",
      "Successfully installed compressed-tensors-0.13.0 huggingface_hub-0.36.2 llmcompressor-0.9.0.1 loguru-0.7.3 nvidia-ml-py-13.590.44 tqdm-4.67.1 transformers-4.57.3\n",
      "Requirement already satisfied: llmcompressor in /usr/local/lib/python3.12/dist-packages (0.9.0.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: loguru<=0.7.3,>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (0.7.3)\n",
      "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
      "Requirement already satisfied: numpy<=2.3.5,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
      "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
      "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
      "Requirement already satisfied: torch<=2.9.1,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.9.0+cu128)\n",
      "Requirement already satisfied: datasets<=4.4.1,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.0.0)\n",
      "Requirement already satisfied: accelerate<=1.12.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.12.0)\n",
      "Requirement already satisfied: nvidia-ml-py<=13.590.44,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (13.590.44)\n",
      "Requirement already satisfied: pillow<=12.0.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (0.13.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.13.0->llmcompressor) (2.12.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.12.0,>=1.6.0->llmcompressor) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<=4.4.1,>=4.0.0->llmcompressor) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2026.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.9.1,>=2.7.0->llmcompressor) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (3.13.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.13.0->llmcompressor) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.9.1,>=2.7.0->llmcompressor) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.9.1,>=2.7.0->llmcompressor) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<=4.4.1,>=4.0.0->llmcompressor) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install llmcompressor\n",
    "%pip install -U llmcompressor transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93445,
     "status": "ok",
     "timestamp": 1770373738550,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "eu4gLE-sdaMH",
    "outputId": "5a49c275-567e-4ae0-90e6-5c277065668e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770373738556,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "eX190I3Zdnls"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"/content/drive/MyDrive/LGAimers/base_model\"\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 1024\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "CALIBRATION_SEED = 42\n",
    "\n",
    "CALIB_DATASETS = [\n",
    "    {\n",
    "        \"id\": \"LGAI-EXAONE/MANTA-1M\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 512,\n",
    "        \"format\": \"manta\",\n",
    "        \"priority\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"nlpai-lab/kullm-v2\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 256,\n",
    "        \"format\": \"instruction\",\n",
    "        \"priority\": 2,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"heegyu/OIG-small-chip2-ko\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 192,\n",
    "        \"format\": \"oig_human_bot\",\n",
    "        \"priority\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"beomi/KoAlpaca-v1.1a\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 64,\n",
    "        \"format\": \"instruction\",\n",
    "        \"priority\": 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "CALIBRATION_BENCHMARK_EXCLUDE = {\n",
    "    \"LGAI-EXAONE/KoMT-Bench\",\n",
    "    \"LGAI-EXAONE/KMMLU-Redux\",\n",
    "    \"LGAI-EXAONE/KMMLU-Pro\",\n",
    "}\n",
    "KOALPACA_FALLBACK_ID = \"nlpai-lab/kullm-v2\"\n",
    "\n",
    "assert sum(spec[\"n_samples\"] for spec in CALIB_DATASETS) == NUM_CALIBRATION_SAMPLES, (\n",
    "    \"CALIB_DATASETS 샘플 합계와 NUM_CALIBRATION_SAMPLES가 일치해야 합니다.\"\n",
    ")\n",
    "assert not any(spec[\"id\"] in CALIBRATION_BENCHMARK_EXCLUDE for spec in CALIB_DATASETS), (\n",
    "    \"평가용 벤치마크 데이터셋은 calibration에서 제외해야 합니다.\"\n",
    ")\n",
    "\n",
    "PRIMARY_SCHEME = \"W4A16\"\n",
    "FALLBACK_SCHEME = \"W8A8\"\n",
    "\n",
    "ATTN_TARGETS = [\n",
    "    \"re:.*self_attn\\\\.q_proj\",\n",
    "    \"re:.*self_attn\\\\.k_proj\",\n",
    "    \"re:.*self_attn\\\\.v_proj\",\n",
    "    \"re:.*self_attn\\\\.o_proj\",\n",
    "]\n",
    "IGNORE = [\"embed_tokens\", \"lm_head\"]\n",
    "\n",
    "OUT_DIR = \"/content/drive/MyDrive/LGAimers/sq_w4a16_attn_calmix1024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41821,
     "status": "ok",
     "timestamp": 1770373780378,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "6hktjhJAdxQk",
    "outputId": "f1235691-0ce0-457e-aecf-ff492f3881b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 모델 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 모델/토크나이저 로드 완료\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] 모델 로드 중...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"[INFO] 모델/토크나이저 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TARGET] pattern=re:.*self_attn\\.q_proj matches=30 sample=['model.layers.0.self_attn.q_proj', 'model.layers.1.self_attn.q_proj', 'model.layers.2.self_attn.q_proj']\n",
      "[TARGET] pattern=re:.*self_attn\\.k_proj matches=30 sample=['model.layers.0.self_attn.k_proj', 'model.layers.1.self_attn.k_proj', 'model.layers.2.self_attn.k_proj']\n",
      "[TARGET] pattern=re:.*self_attn\\.v_proj matches=30 sample=['model.layers.0.self_attn.v_proj', 'model.layers.1.self_attn.v_proj', 'model.layers.2.self_attn.v_proj']\n",
      "[TARGET] pattern=re:.*self_attn\\.o_proj matches=30 sample=['model.layers.0.self_attn.o_proj', 'model.layers.1.self_attn.o_proj', 'model.layers.2.self_attn.o_proj']\n",
      "[TARGET] total_unique_matches=120\n",
      "[TARGET] layer_count=30, layer_range=(0, 29)\n",
      "[INFO] attention-only target validation complete\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def collect_target_modules(model, target_patterns):\n",
    "    all_module_names = [name for name, _ in model.named_modules()]\n",
    "\n",
    "    matched_by_pattern = {}\n",
    "    for pattern in target_patterns:\n",
    "        if pattern.startswith(\"re:\"):\n",
    "            regex = pattern[len(\"re:\"):]\n",
    "        else:\n",
    "            regex = re.escape(pattern)\n",
    "        matches = [name for name in all_module_names if re.fullmatch(regex, name)]\n",
    "        matched_by_pattern[pattern] = matches\n",
    "        print(f\"[TARGET] pattern={pattern} matches={len(matches)} sample={matches[:3]}\")\n",
    "\n",
    "    unmatched_patterns = [p for p, m in matched_by_pattern.items() if not m]\n",
    "    if unmatched_patterns:\n",
    "        raise AssertionError(f\"No module matched for patterns: {unmatched_patterns}\")\n",
    "\n",
    "    unique_matches = sorted({m for matches in matched_by_pattern.values() for m in matches})\n",
    "    mlp_overlap = [name for name in unique_matches if \".mlp.\" in name]\n",
    "    if mlp_overlap:\n",
    "        raise AssertionError(f\"MLP modules were matched unexpectedly: {mlp_overlap[:10]}\")\n",
    "\n",
    "    expected_suffixes = {\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "    }\n",
    "    per_layer = {}\n",
    "    for name in unique_matches:\n",
    "        parts = name.split(\".\")\n",
    "        if len(parts) < 5 or parts[0] != \"model\" or parts[1] != \"layers\" or not parts[2].isdigit():\n",
    "            continue\n",
    "        layer_id = int(parts[2])\n",
    "        suffix = \".\".join(parts[3:])\n",
    "        per_layer.setdefault(layer_id, set()).add(suffix)\n",
    "\n",
    "    missing_per_layer = {\n",
    "        layer_id: sorted(expected_suffixes - suffixes)\n",
    "        for layer_id, suffixes in per_layer.items()\n",
    "        if suffixes != expected_suffixes\n",
    "    }\n",
    "    if missing_per_layer:\n",
    "        raise AssertionError(f\"Incomplete attention targets per layer: {missing_per_layer}\")\n",
    "\n",
    "    expected_total = 120\n",
    "    if len(unique_matches) != expected_total:\n",
    "        raise AssertionError(\n",
    "            f\"Expected {expected_total} attention modules (30 layers x 4 projections), got {len(unique_matches)}\"\n",
    "        )\n",
    "\n",
    "    layer_ids = sorted(per_layer)\n",
    "    print(f\"[TARGET] total_unique_matches={len(unique_matches)}\")\n",
    "    print(f\"[TARGET] layer_count={len(layer_ids)}, layer_range=({layer_ids[0]}, {layer_ids[-1]})\")\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "\n",
    "ATTN_MATCHED_MODULES = collect_target_modules(model, ATTN_TARGETS)\n",
    "print(\"[INFO] attention-only target validation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312,
     "referenced_widgets": [
      "b299e23224d24d3eb776cfcf092af1c2",
      "3020f237b81f4e1c826641043303009a",
      "a81140f334f941f18aa669efe7c3482f",
      "2da239963de44ed8b4b80ef5cefdc963",
      "1b21e94a4d6f4bf38bd1cc184336a4fc",
      "638af45d596f44b4924ab8bc7640b5cf",
      "7691ddbb6258474ba3b413e8cc77e067",
      "ec8115a116764bc898b1050d1b206f1b",
      "8158348ca062439e8340097dcd662ffa",
      "3142147fb68e46a29cfc7c116bb2cc90",
      "61b99d17ff694de5be15ace4032607f5",
      "a06c20cad5a741e385844d0a9efddb05",
      "374b9cb109f64f04ad77df5e0d9b6acc",
      "9f967f2336724d5bb8fa304acfc0172b",
      "dfc9b159922642a8a20a8527d02b6c63",
      "bbc9a0f5def74f178f8351cfb14a4184",
      "90562e41607d44309ea17e4840a6f091",
      "0c41977b3e84448fa63c86867d02dd51",
      "c221d9ea204f44e489f7d2b1103e1f64",
      "f08a8bcc4ad7495380a16a75ed2a821a",
      "ccd58b180caa45b287381db67ed859f2",
      "00cce46a3062432f8f0ef1e0cb613c39",
      "6799fa726f7040bb8798a7cacdae656b",
      "56e74919f40d475fb7f313484e1b3b90",
      "a9322ea1cf7f482ca1915fb8f117db6b",
      "e56a5099024b428096888d69458fcf71",
      "c24d532dd9214848841889eb577f7e79",
      "133a1ff3125f40c09a076054a4f19d5f",
      "ef21b4f46d6947198ad5511460712326",
      "f001cf3b5e634d46b9c7770a52355468",
      "0254b1cfd4a14ac88739d68f6b065817",
      "2c0fd9d15c974a468719f159bc6b48b2",
      "8d7aec96dd4a4dc2b1000e87b3b48eb9",
      "22d30c6d3cd547c599be3c177bb0ddc6",
      "572bbd11ea5744588b657b3bce78afb5",
      "1eda342391f649d483703d939f3ade3d",
      "93fb5465bcf145949735038460467d2f",
      "99a42dc7ecb24e88970ddc96970e388e",
      "432ee61f2d0c49cb8c6a9ec540757d6e",
      "4567c4dac35948ce97fcb3b556e44b9a",
      "1ce6af7aa75b4d0194d8f36a480517e5",
      "0cfdcef81fcb4e88929e69e4e7fa23d8",
      "0855c5ccf25d43ac9dd33c89425b6dde",
      "8a0baf40b6534763a2e2c2c101efa1fe"
     ]
    },
    "executionInfo": {
     "elapsed": 43270,
     "status": "ok",
     "timestamp": 1770373823651,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "lltC2dq1dzEf",
    "outputId": "aea07327-f045-4f64-f9cd-c7be77782c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 캘리브레이션 데이터 로드 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056fb9182d63442ca570b3c0c97ef438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af66ee827a9a4a17ba4a3c08d3bdff57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-21df739eb88d71(…):   0%|          | 0.00/12.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaf060d90544c52a388811094044c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 혼합 캘리브레이션 샘플 수: 1024\n",
      "[INFO] 한글 문자 비율: 0.0666\n",
      "[INFO] 한글 포함 샘플 비율: 0.4980\n",
      "[INFO] 소스별 통계:\n",
      "  - {'id': 'LGAI-EXAONE/MANTA-1M', 'split': 'train', 'format': 'manta', 'target_n': 512, 'selected_n': 512, 'scanned_n': 512, 'dropped_n': 0, 'duplicate_n': 0, 'columns': ['id', 'conversations', 'complexity_label']}\n",
      "  - {'id': 'nlpai-lab/kullm-v2', 'split': 'train', 'format': 'instruction', 'target_n': 256, 'selected_n': 256, 'scanned_n': 257, 'dropped_n': 1, 'duplicate_n': 0, 'columns': ['id', 'instruction', 'input', 'output']}\n",
      "  - {'id': 'heegyu/OIG-small-chip2-ko', 'split': 'train', 'format': 'oig_human_bot', 'target_n': 192, 'selected_n': 192, 'scanned_n': 192, 'dropped_n': 0, 'duplicate_n': 0, 'columns': ['user', 'chip2', 'index', 'user_translated', 'chip2_translated']}\n",
      "  - {'id': 'beomi/KoAlpaca-v1.1a', 'split': 'train', 'format': 'instruction', 'target_n': 64, 'selected_n': 64, 'scanned_n': 64, 'dropped_n': 0, 'duplicate_n': 0, 'columns': ['instruction', 'output', 'url']}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935c564597374ac788e9b2d0f9bb36fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 데이터 전처리 완료\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] 캘리브레이션 데이터 로드 중...\")\n",
    "\n",
    "def _clean_text(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "def _normalize_manta_conversations(conversations):\n",
    "    if not isinstance(conversations, list):\n",
    "        return None\n",
    "\n",
    "    normalized = []\n",
    "    for turn in conversations:\n",
    "        if not isinstance(turn, dict):\n",
    "            continue\n",
    "        role = _clean_text(turn.get(\"role\")).lower()\n",
    "        content = _clean_text(turn.get(\"content\"))\n",
    "        if role in {\"system\", \"user\", \"assistant\"} and content:\n",
    "            normalized.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    has_user = any(turn[\"role\"] == \"user\" for turn in normalized)\n",
    "    has_assistant = any(turn[\"role\"] == \"assistant\" for turn in normalized)\n",
    "    if not (has_user and has_assistant):\n",
    "        return None\n",
    "    return normalized\n",
    "\n",
    "def _normalize_instruction_turn(example):\n",
    "    instruction = _clean_text(example.get(\"instruction\"))\n",
    "    input_text = _clean_text(example.get(\"input\"))\n",
    "    output_text = _clean_text(example.get(\"output\"))\n",
    "\n",
    "    user_content = instruction\n",
    "    if input_text:\n",
    "        user_content = f\"{instruction}\\n\\n{input_text}\" if instruction else input_text\n",
    "\n",
    "    if not user_content or not output_text:\n",
    "        return None\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": output_text},\n",
    "    ]\n",
    "\n",
    "def _normalize_oig_pair(example):\n",
    "    # heegyu/OIG-small-chip2-ko는 user/chip2 또는 *_translated 컬럼 기반이다.\n",
    "    user_content = _clean_text(example.get(\"user_translated\")) or _clean_text(example.get(\"user\"))\n",
    "    assistant_content = _clean_text(example.get(\"chip2_translated\")) or _clean_text(example.get(\"chip2\"))\n",
    "    if user_content and assistant_content:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "        ]\n",
    "\n",
    "    # 구버전/파생셋 호환: text 컬럼에 <human>/<bot> 태그가 있는 경우 파싱.\n",
    "    raw = _clean_text(example.get(\"text\"))\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    pattern = re.compile(r\"<human>\\s*(.*?)\\s*<bot>\\s*(.*?)(?=<human>|$)\", re.IGNORECASE | re.DOTALL)\n",
    "    match = pattern.search(raw)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    user_content = _clean_text(match.group(1))\n",
    "    assistant_content = _clean_text(match.group(2))\n",
    "    if not user_content or not assistant_content:\n",
    "        return None\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "    ]\n",
    "\n",
    "def normalize_example(example, source_format):\n",
    "    if source_format == \"manta\":\n",
    "        conversations = _normalize_manta_conversations(example.get(\"conversations\"))\n",
    "    elif source_format == \"instruction\":\n",
    "        conversations = _normalize_instruction_turn(example)\n",
    "    elif source_format == \"oig_human_bot\":\n",
    "        conversations = _normalize_oig_pair(example)\n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 source format: {source_format}\")\n",
    "\n",
    "    if conversations is None:\n",
    "        return None\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def _conversation_hash(record):\n",
    "    payload = json.dumps(record[\"conversations\"], ensure_ascii=False, sort_keys=True)\n",
    "    return hashlib.sha1(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _load_single_spec(spec, seed, global_seen_hashes):\n",
    "    dataset_id = spec[\"id\"]\n",
    "    split = spec[\"split\"]\n",
    "    target_n = spec[\"n_samples\"]\n",
    "    source_format = spec[\"format\"]\n",
    "\n",
    "    raw_ds = load_dataset(dataset_id, split=split).shuffle(seed=seed)\n",
    "    dataset_columns = list(raw_ds.features.keys())\n",
    "\n",
    "    selected = []\n",
    "    scanned = 0\n",
    "    dropped = 0\n",
    "    duplicate = 0\n",
    "\n",
    "    for example in raw_ds:\n",
    "        scanned += 1\n",
    "        normalized = normalize_example(example, source_format)\n",
    "        if normalized is None:\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        conv_hash = _conversation_hash(normalized)\n",
    "        if conv_hash in global_seen_hashes:\n",
    "            duplicate += 1\n",
    "            continue\n",
    "\n",
    "        global_seen_hashes.add(conv_hash)\n",
    "        selected.append(normalized)\n",
    "        if len(selected) >= target_n:\n",
    "            break\n",
    "\n",
    "    if len(selected) < target_n:\n",
    "        raise RuntimeError(\n",
    "            f\"{dataset_id}에서 목표 샘플({target_n})을 채우지 못했습니다. \"\n",
    "            f\"selected={len(selected)}, scanned={scanned}, dropped={dropped}, duplicate={duplicate}, \"\n",
    "            f\"columns={dataset_columns}\"\n",
    "        )\n",
    "\n",
    "    normalized_ds = Dataset.from_list(selected)\n",
    "    stats = {\n",
    "        \"id\": dataset_id,\n",
    "        \"split\": split,\n",
    "        \"format\": source_format,\n",
    "        \"target_n\": target_n,\n",
    "        \"selected_n\": len(selected),\n",
    "        \"scanned_n\": scanned,\n",
    "        \"dropped_n\": dropped,\n",
    "        \"duplicate_n\": duplicate,\n",
    "        \"columns\": dataset_columns,\n",
    "    }\n",
    "    return normalized_ds, stats\n",
    "\n",
    "def build_calibration_dataset(specs, seed):\n",
    "    if not specs:\n",
    "        raise ValueError(\"CALIB_DATASETS가 비어 있습니다.\")\n",
    "\n",
    "    specs_sorted = sorted(specs, key=lambda x: x[\"priority\"])\n",
    "    subset_list = []\n",
    "    stats_list = []\n",
    "    global_seen_hashes = set()\n",
    "\n",
    "    for spec in specs_sorted:\n",
    "        try:\n",
    "            subset, stats = _load_single_spec(spec, seed=seed, global_seen_hashes=global_seen_hashes)\n",
    "            subset_list.append(subset)\n",
    "            stats_list.append(stats)\n",
    "        except Exception as exc:\n",
    "            if spec[\"id\"] == \"beomi/KoAlpaca-v1.1a\":\n",
    "                print(f\"[WARN] {spec['id']} 로드 실패 -> {KOALPACA_FALLBACK_ID}로 대체: {type(exc).__name__}: {exc}\")\n",
    "                fallback_spec = dict(spec)\n",
    "                fallback_spec[\"id\"] = KOALPACA_FALLBACK_ID\n",
    "                fallback_spec[\"split\"] = \"train\"\n",
    "                fallback_spec[\"format\"] = \"instruction\"\n",
    "                subset, stats = _load_single_spec(fallback_spec, seed=seed, global_seen_hashes=global_seen_hashes)\n",
    "                stats[\"fallback_from\"] = spec[\"id\"]\n",
    "                subset_list.append(subset)\n",
    "                stats_list.append(stats)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "    mixed = concatenate_datasets(subset_list).shuffle(seed=seed)\n",
    "\n",
    "    if len(mixed) != NUM_CALIBRATION_SAMPLES:\n",
    "        raise RuntimeError(\n",
    "            f\"최종 calibration 샘플 수가 다릅니다: expected={NUM_CALIBRATION_SAMPLES}, actual={len(mixed)}\"\n",
    "        )\n",
    "\n",
    "    for stats in stats_list:\n",
    "        if stats[\"selected_n\"] < min(5, stats[\"target_n\"]):\n",
    "            raise RuntimeError(f\"정규화 성공 샘플이 부족합니다: {stats}\")\n",
    "\n",
    "    return mixed, stats_list\n",
    "\n",
    "def _contains_korean(text):\n",
    "    return any(\"가\" <= ch <= \"힣\" for ch in text)\n",
    "\n",
    "def _compute_ko_char_ratio(records):\n",
    "    total_chars = 0\n",
    "    ko_chars = 0\n",
    "    for record in records:\n",
    "        for turn in record[\"conversations\"]:\n",
    "            content = turn[\"content\"]\n",
    "            total_chars += len(content)\n",
    "            ko_chars += sum(1 for ch in content if \"가\" <= ch <= \"힣\")\n",
    "    if total_chars == 0:\n",
    "        return 0.0\n",
    "    return ko_chars / total_chars\n",
    "\n",
    "ds, calibration_source_stats = build_calibration_dataset(CALIB_DATASETS, seed=CALIBRATION_SEED)\n",
    "\n",
    "records = list(ds)\n",
    "ko_char_ratio = _compute_ko_char_ratio(records)\n",
    "ko_record_ratio = (\n",
    "    sum(1 for item in records if _contains_korean(json.dumps(item[\"conversations\"], ensure_ascii=False)))\n",
    "    / max(len(records), 1)\n",
    ")\n",
    "\n",
    "print(f\"[INFO] 혼합 캘리브레이션 샘플 수: {len(records)}\")\n",
    "print(f\"[INFO] 한글 문자 비율: {ko_char_ratio:.4f}\")\n",
    "print(f\"[INFO] 한글 포함 샘플 비율: {ko_record_ratio:.4f}\")\n",
    "print(\"[INFO] 소스별 통계:\")\n",
    "for stats in calibration_source_stats:\n",
    "    print(f\"  - {stats}\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "print(\"[INFO] 데이터 전처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664,
     "referenced_widgets": [
      "7a83da2058af4b1ba16cbc1642a9d870",
      "716daef2903a4b26b94f0dacd93ec439",
      "b6b1c94a9abb4a8aa4b328ec027f9b3b",
      "79b5c92fd18b413f8a05ef48b19ea1c3",
      "35912d36448a488cb576eeb5f2e043b6",
      "a6614d2c5dd248038efacb895268f37b",
      "bcc86fd3404f42eab24f1b3b84d0a84d",
      "617a008fc86e427285d30004a9887ca1",
      "4dbb9d4a86284fe8b680f240de006a79",
      "955b76360c58473a835cf96e2f1b53a8",
      "e55f82a8245d4b359225d7dde42ef682"
     ]
    },
    "executionInfo": {
     "elapsed": 1361,
     "status": "error",
     "timestamp": 1770373825018,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "uOY47Ppyd3I2",
    "outputId": "4dec6402-ea60-475e-f5fc-685f7bfc3d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPTQ(attention-only) start (1/2, scheme=W4A16, samples=1024, max_len=512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cf7f6a89144b8d9ec93cf05c1859fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:30:50.224497+0000 | reset | INFO - Compression lifecycle reset\n",
      "2026-02-13T18:30:50.227398+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-02-13T18:30:50.266046+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2026-02-13T18:30:50.266996+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `GPTQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 1024/1024 [00:00<00:00, 1201.20it/s]\n",
      "(1/31): Calibrating: 100%|██████████| 1024/1024 [00:06<00:00, 159.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:04.371620+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:06.160079+0000 | compress | METRIC - time 1.79s\n",
      "2026-02-13T18:31:06.160967+0000 | compress | METRIC - error 0.84\n",
      "2026-02-13T18:31:06.162266+0000 | compress | METRIC - GPU 0 | usage: 2.39% | total memory: 85 GB\n",
      "2026-02-13T18:31:06.162881+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:31:06.163976+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:31:07.345169+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:31:07.346274+0000 | compress | METRIC - error 0.24\n",
      "2026-02-13T18:31:07.347031+0000 | compress | METRIC - GPU 0 | usage: 2.39% | total memory: 85 GB\n",
      "2026-02-13T18:31:07.347584+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:07.348904+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:31:08.536577+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:31:08.537842+0000 | compress | METRIC - error 0.14\n",
      "2026-02-13T18:31:08.538922+0000 | compress | METRIC - GPU 0 | usage: 2.39% | total memory: 85 GB\n",
      "2026-02-13T18:31:08.539535+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:08.540922+0000 | compress_modules | INFO - Quantizing model.layers.0.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:31:09.737526+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:31:09.738680+0000 | compress | METRIC - error 0.03\n",
      "2026-02-13T18:31:09.739602+0000 | compress | METRIC - GPU 0 | usage: 2.39% | total memory: 85 GB\n",
      "2026-02-13T18:31:09.740219+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1/31): Propagating: 100%|██████████| 1024/1024 [00:09<00:00, 102.60it/s]\n",
      "(2/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 299.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:23.465771+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:24.657222+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:31:24.658372+0000 | compress | METRIC - error 3.98\n",
      "2026-02-13T18:31:24.659421+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:24.660177+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:31:24.661447+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:31:25.839330+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:31:25.840508+0000 | compress | METRIC - error 1.14\n",
      "2026-02-13T18:31:25.841415+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:25.841928+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:25.842892+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:31:27.045874+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:31:27.047154+0000 | compress | METRIC - error 1.06\n",
      "2026-02-13T18:31:27.048122+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:27.048809+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:27.050119+0000 | compress_modules | INFO - Quantizing model.layers.1.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:31:28.266858+0000 | compress | METRIC - time 1.22s\n",
      "2026-02-13T18:31:28.267760+0000 | compress | METRIC - error 0.18\n",
      "2026-02-13T18:31:28.268658+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:28.269435+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(2/31): Propagating: 100%|██████████| 1024/1024 [00:04<00:00, 250.13it/s]\n",
      "(3/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 297.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:35.865422+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:37.070686+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:31:37.072222+0000 | compress | METRIC - error 10.89\n",
      "2026-02-13T18:31:37.072891+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:37.073458+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:31:37.074425+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:31:38.257655+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:31:38.259075+0000 | compress | METRIC - error 3.08\n",
      "2026-02-13T18:31:38.260093+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:38.260930+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:38.261824+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:31:39.451058+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:31:39.452511+0000 | compress | METRIC - error 3.00\n",
      "2026-02-13T18:31:39.453306+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:39.453960+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:39.455097+0000 | compress_modules | INFO - Quantizing model.layers.2.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:31:40.650699+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:31:40.652207+0000 | compress | METRIC - error 0.46\n",
      "2026-02-13T18:31:40.653349+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:40.653992+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(3/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 317.33it/s]\n",
      "(4/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 297.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:47.367097+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:48.584173+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:31:48.585148+0000 | compress | METRIC - error 21.66\n",
      "2026-02-13T18:31:48.586002+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:48.586538+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:31:48.587484+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:31:49.758568+0000 | compress | METRIC - time 1.17s\n",
      "2026-02-13T18:31:49.760156+0000 | compress | METRIC - error 6.15\n",
      "2026-02-13T18:31:49.761274+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:49.761876+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:49.762926+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:31:50.951674+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:31:50.953250+0000 | compress | METRIC - error 5.48\n",
      "2026-02-13T18:31:50.954557+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:50.955207+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:31:50.956293+0000 | compress_modules | INFO - Quantizing model.layers.3.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:31:52.171722+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:31:52.173256+0000 | compress | METRIC - error 0.39\n",
      "2026-02-13T18:31:52.174087+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:31:52.174674+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(4/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 315.97it/s]\n",
      "(5/31): Calibrating: 100%|██████████| 1024/1024 [00:04<00:00, 245.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:31:59.634097+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:00.846181+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:32:00.847127+0000 | compress | METRIC - error 41.24\n",
      "2026-02-13T18:32:00.848068+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:00.848779+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:00.849779+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:02.038431+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:32:02.040032+0000 | compress | METRIC - error 11.47\n",
      "2026-02-13T18:32:02.041087+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:02.041742+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:02.042848+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:32:03.228021+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:32:03.229665+0000 | compress | METRIC - error 10.44\n",
      "2026-02-13T18:32:03.230707+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:03.231330+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:03.232628+0000 | compress_modules | INFO - Quantizing model.layers.4.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:32:04.443285+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:32:04.444865+0000 | compress | METRIC - error 1.08\n",
      "2026-02-13T18:32:04.445757+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:04.446319+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(5/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.26it/s]\n",
      "(6/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 297.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:11.204097+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:12.417636+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:32:12.419425+0000 | compress | METRIC - error 66.28\n",
      "2026-02-13T18:32:12.420310+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:12.420944+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:12.422003+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:13.614943+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:32:13.616750+0000 | compress | METRIC - error 19.55\n",
      "2026-02-13T18:32:13.617708+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:13.618384+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:13.619393+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:32:14.809124+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:32:14.810851+0000 | compress | METRIC - error 16.87\n",
      "2026-02-13T18:32:14.811682+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:14.812311+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:14.813522+0000 | compress_modules | INFO - Quantizing model.layers.5.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:32:16.022293+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:32:16.023994+0000 | compress | METRIC - error 1.99\n",
      "2026-02-13T18:32:16.024860+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:16.025582+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(6/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 311.84it/s]\n",
      "(7/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 296.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:22.793824+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:23.996541+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:23.998401+0000 | compress | METRIC - error 99.12\n",
      "2026-02-13T18:32:23.999212+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:23.999822+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:24.000907+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:25.197582+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:25.199401+0000 | compress | METRIC - error 27.36\n",
      "2026-02-13T18:32:25.200163+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:25.200756+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:25.201783+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:32:26.398016+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:26.399861+0000 | compress | METRIC - error 27.17\n",
      "2026-02-13T18:32:26.400816+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:26.401492+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:26.402528+0000 | compress_modules | INFO - Quantizing model.layers.6.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:32:27.599408+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:27.601281+0000 | compress | METRIC - error 6.32\n",
      "2026-02-13T18:32:27.602125+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:27.602747+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(7/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 314.12it/s]\n",
      "(8/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 296.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:34.350893+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:35.542562+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:32:35.544486+0000 | compress | METRIC - error 150.90\n",
      "2026-02-13T18:32:35.545263+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:35.545801+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:35.546687+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:36.705302+0000 | compress | METRIC - time 1.16s\n",
      "2026-02-13T18:32:36.707111+0000 | compress | METRIC - error 42.49\n",
      "2026-02-13T18:32:36.708012+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:36.708696+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:36.709838+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:32:37.895301+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:32:37.897151+0000 | compress | METRIC - error 38.16\n",
      "2026-02-13T18:32:37.898037+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:37.898823+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:37.899730+0000 | compress_modules | INFO - Quantizing model.layers.7.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:32:39.107278+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:32:39.109117+0000 | compress | METRIC - error 7.63\n",
      "2026-02-13T18:32:39.109798+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:39.110269+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(8/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 311.66it/s]\n",
      "(9/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 297.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:45.879002+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:47.078951+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:47.080806+0000 | compress | METRIC - error 166.53\n",
      "2026-02-13T18:32:47.081656+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:47.082266+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:47.083395+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:48.262435+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:32:48.264323+0000 | compress | METRIC - error 47.65\n",
      "2026-02-13T18:32:48.265171+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:48.265858+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:48.266934+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:32:49.460058+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:32:49.461908+0000 | compress | METRIC - error 47.24\n",
      "2026-02-13T18:32:49.462741+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:49.463308+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:49.464342+0000 | compress_modules | INFO - Quantizing model.layers.8.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:32:50.667793+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:32:50.669666+0000 | compress | METRIC - error 8.11\n",
      "2026-02-13T18:32:50.670463+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:50.670986+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(9/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 310.25it/s]\n",
      "(10/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 298.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:57.444666+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:32:58.632130+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:32:58.634002+0000 | compress | METRIC - error 224.07\n",
      "2026-02-13T18:32:58.634952+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:58.635950+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:32:58.637081+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:32:59.796333+0000 | compress | METRIC - time 1.16s\n",
      "2026-02-13T18:32:59.798264+0000 | compress | METRIC - error 66.40\n",
      "2026-02-13T18:32:59.799275+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:32:59.800166+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:32:59.801096+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:33:00.993031+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:33:00.994892+0000 | compress | METRIC - error 64.69\n",
      "2026-02-13T18:33:00.995804+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:00.996428+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:00.997638+0000 | compress_modules | INFO - Quantizing model.layers.9.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:33:02.181868+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:02.183929+0000 | compress | METRIC - error 11.74\n",
      "2026-02-13T18:33:02.184996+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:02.185729+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(10/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 317.51it/s]\n",
      "(11/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 266.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:09.293183+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:10.498300+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:33:10.500253+0000 | compress | METRIC - error 244.42\n",
      "2026-02-13T18:33:10.500993+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:10.501631+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:33:10.502947+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:33:11.682395+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:11.684353+0000 | compress | METRIC - error 66.12\n",
      "2026-02-13T18:33:11.685202+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:11.685857+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:11.686924+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:33:12.875343+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:33:12.877286+0000 | compress | METRIC - error 69.81\n",
      "2026-02-13T18:33:12.878182+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:12.878837+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:12.879867+0000 | compress_modules | INFO - Quantizing model.layers.10.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:33:14.081107+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:33:14.083040+0000 | compress | METRIC - error 10.20\n",
      "2026-02-13T18:33:14.083841+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:14.084427+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(11/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 315.01it/s]\n",
      "(12/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 295.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:20.842161+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:22.042353+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:33:22.044410+0000 | compress | METRIC - error 267.66\n",
      "2026-02-13T18:33:22.045329+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:22.045879+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:33:22.046981+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:33:23.222808+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:23.224855+0000 | compress | METRIC - error 75.95\n",
      "2026-02-13T18:33:23.225792+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:23.226361+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:23.227336+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:33:24.422783+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:33:24.424773+0000 | compress | METRIC - error 80.57\n",
      "2026-02-13T18:33:24.425726+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:24.426349+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:24.427615+0000 | compress_modules | INFO - Quantizing model.layers.11.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:33:25.642694+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:33:25.644616+0000 | compress | METRIC - error 15.48\n",
      "2026-02-13T18:33:25.645428+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:25.645920+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(12/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 311.87it/s]\n",
      "(13/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 299.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:32.395441+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:33.606025+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:33:33.607950+0000 | compress | METRIC - error 298.17\n",
      "2026-02-13T18:33:33.608790+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:33.609283+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:33:33.610311+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:33:34.793831+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:34.795760+0000 | compress | METRIC - error 81.96\n",
      "2026-02-13T18:33:34.796835+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:34.797546+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:34.798694+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:33:35.987026+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:33:35.988970+0000 | compress | METRIC - error 85.28\n",
      "2026-02-13T18:33:35.989929+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:35.990576+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:35.991847+0000 | compress_modules | INFO - Quantizing model.layers.12.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:33:37.198949+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:33:37.200892+0000 | compress | METRIC - error 11.86\n",
      "2026-02-13T18:33:37.201834+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:37.202458+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(13/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 314.07it/s]\n",
      "(14/31): Calibrating: 100%|██████████| 1024/1024 [00:04<00:00, 233.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:44.887000+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:46.098390+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:33:46.100399+0000 | compress | METRIC - error 334.42\n",
      "2026-02-13T18:33:46.101193+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:46.101682+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:33:46.102791+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:33:47.283023+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:47.284984+0000 | compress | METRIC - error 94.26\n",
      "2026-02-13T18:33:47.285867+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:47.286432+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:47.287414+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:33:48.468659+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:33:48.470664+0000 | compress | METRIC - error 128.63\n",
      "2026-02-13T18:33:48.471558+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:48.472096+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:48.473069+0000 | compress_modules | INFO - Quantizing model.layers.13.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:33:49.677377+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:33:49.679438+0000 | compress | METRIC - error 42.09\n",
      "2026-02-13T18:33:49.680177+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:49.680971+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(14/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.47it/s]\n",
      "(15/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 292.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:56.486254+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:33:57.676118+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:33:57.678541+0000 | compress | METRIC - error 365.60\n",
      "2026-02-13T18:33:57.679445+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:57.680076+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:33:57.681246+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:33:58.884269+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:33:58.885569+0000 | compress | METRIC - error 111.05\n",
      "2026-02-13T18:33:58.886345+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:33:58.886823+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:33:58.887706+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:00.080679+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:00.082814+0000 | compress | METRIC - error 103.37\n",
      "2026-02-13T18:34:00.083631+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:00.084291+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:00.085664+0000 | compress_modules | INFO - Quantizing model.layers.14.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:34:01.289272+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:34:01.291354+0000 | compress | METRIC - error 23.37\n",
      "2026-02-13T18:34:01.292543+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:01.293190+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(15/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 315.04it/s]\n",
      "(16/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 298.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:08.014792+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:09.227788+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:34:09.229751+0000 | compress | METRIC - error 373.40\n",
      "2026-02-13T18:34:09.230571+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:09.231166+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:34:09.232123+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:34:10.410651+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:34:10.412668+0000 | compress | METRIC - error 106.04\n",
      "2026-02-13T18:34:10.413487+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:10.413926+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:10.415074+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:11.610496+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:11.612508+0000 | compress | METRIC - error 105.37\n",
      "2026-02-13T18:34:11.613397+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:11.613882+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:11.614847+0000 | compress_modules | INFO - Quantizing model.layers.15.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:34:12.824713+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:34:12.826659+0000 | compress | METRIC - error 16.09\n",
      "2026-02-13T18:34:12.827535+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:12.828066+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(16/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 315.75it/s]\n",
      "(17/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 293.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:19.595372+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:20.792199+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:20.794173+0000 | compress | METRIC - error 441.48\n",
      "2026-02-13T18:34:20.795082+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:20.795701+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:34:20.796449+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:34:21.997208+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:34:21.999202+0000 | compress | METRIC - error 116.54\n",
      "2026-02-13T18:34:22.000042+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:22.000599+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:22.001507+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:23.191590+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:23.193591+0000 | compress | METRIC - error 116.15\n",
      "2026-02-13T18:34:23.194378+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:23.194876+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:23.196196+0000 | compress_modules | INFO - Quantizing model.layers.16.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:34:24.389254+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:24.391199+0000 | compress | METRIC - error 13.82\n",
      "2026-02-13T18:34:24.392065+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:24.392686+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(17/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 309.65it/s]\n",
      "(18/31): Calibrating: 100%|██████████| 1024/1024 [00:04<00:00, 234.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:32.098280+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:33.306398+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:34:33.308465+0000 | compress | METRIC - error 448.93\n",
      "2026-02-13T18:34:33.309249+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:33.309798+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:34:33.310760+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:34:34.490062+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:34:34.492200+0000 | compress | METRIC - error 122.78\n",
      "2026-02-13T18:34:34.493097+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:34.493786+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:34.494809+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:35.666678+0000 | compress | METRIC - time 1.17s\n",
      "2026-02-13T18:34:35.668642+0000 | compress | METRIC - error 138.46\n",
      "2026-02-13T18:34:35.669496+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:35.670059+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:35.671252+0000 | compress_modules | INFO - Quantizing model.layers.17.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:34:36.875877+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:34:36.877836+0000 | compress | METRIC - error 11.56\n",
      "2026-02-13T18:34:36.878722+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:36.879333+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(18/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 312.00it/s]\n",
      "(19/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 296.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:43.651766+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:44.852302+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:34:44.854306+0000 | compress | METRIC - error 491.02\n",
      "2026-02-13T18:34:44.855078+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:44.855584+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:34:44.856454+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:34:46.031190+0000 | compress | METRIC - time 1.17s\n",
      "2026-02-13T18:34:46.033254+0000 | compress | METRIC - error 140.95\n",
      "2026-02-13T18:34:46.034090+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:46.034813+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:46.035691+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:47.216486+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:34:47.218521+0000 | compress | METRIC - error 137.87\n",
      "2026-02-13T18:34:47.219339+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:47.219826+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:47.220781+0000 | compress_modules | INFO - Quantizing model.layers.18.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:34:48.426947+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:34:48.428907+0000 | compress | METRIC - error 18.74\n",
      "2026-02-13T18:34:48.429761+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:48.430324+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(19/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.45it/s]\n",
      "(20/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 295.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:55.210210+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:34:56.441746+0000 | compress | METRIC - time 1.23s\n",
      "2026-02-13T18:34:56.443784+0000 | compress | METRIC - error 492.01\n",
      "2026-02-13T18:34:56.444637+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:56.445132+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:34:56.446278+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:34:57.632673+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:34:57.634671+0000 | compress | METRIC - error 141.50\n",
      "2026-02-13T18:34:57.635593+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:57.636196+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:57.637355+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:34:58.818555+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:34:58.820673+0000 | compress | METRIC - error 153.78\n",
      "2026-02-13T18:34:58.821727+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:34:58.822525+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:34:58.823641+0000 | compress_modules | INFO - Quantizing model.layers.19.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:00.043420+0000 | compress | METRIC - time 1.22s\n",
      "2026-02-13T18:35:00.045463+0000 | compress | METRIC - error 25.19\n",
      "2026-02-13T18:35:00.046359+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:00.046923+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(20/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.78it/s]\n",
      "(21/31): Calibrating: 100%|██████████| 1024/1024 [00:04<00:00, 235.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:07.696204+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:08.932251+0000 | compress | METRIC - time 1.23s\n",
      "2026-02-13T18:35:08.934195+0000 | compress | METRIC - error 577.14\n",
      "2026-02-13T18:35:08.935054+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:08.935641+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:35:08.936826+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:35:10.121083+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:35:10.122981+0000 | compress | METRIC - error 155.42\n",
      "2026-02-13T18:35:10.123769+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:10.124319+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:10.125507+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:35:11.315671+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:11.317662+0000 | compress | METRIC - error 175.17\n",
      "2026-02-13T18:35:11.318456+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:11.319040+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:11.320001+0000 | compress_modules | INFO - Quantizing model.layers.20.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:12.524341+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:35:12.526320+0000 | compress | METRIC - error 26.49\n",
      "2026-02-13T18:35:12.527091+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:12.527677+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(21/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.16it/s]\n",
      "(22/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 295.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:19.304988+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:20.536271+0000 | compress | METRIC - time 1.23s\n",
      "2026-02-13T18:35:20.538295+0000 | compress | METRIC - error 659.29\n",
      "2026-02-13T18:35:20.539130+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:20.539715+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:35:20.540762+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:35:21.717433+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:35:21.719465+0000 | compress | METRIC - error 178.43\n",
      "2026-02-13T18:35:21.720376+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:21.720840+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:21.721784+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:35:22.910732+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:22.912731+0000 | compress | METRIC - error 178.29\n",
      "2026-02-13T18:35:22.913737+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:22.914337+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:22.915438+0000 | compress_modules | INFO - Quantizing model.layers.21.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:24.124416+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:35:24.126790+0000 | compress | METRIC - error 25.44\n",
      "2026-02-13T18:35:24.127883+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:24.128364+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(22/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 312.27it/s]\n",
      "(23/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 296.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:30.900735+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:32.115868+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:35:32.117844+0000 | compress | METRIC - error 712.07\n",
      "2026-02-13T18:35:32.118727+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:32.119332+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:35:32.120386+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:35:33.324369+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:35:33.326421+0000 | compress | METRIC - error 203.20\n",
      "2026-02-13T18:35:33.327315+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:33.327860+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:33.328795+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:35:34.522458+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:34.524634+0000 | compress | METRIC - error 232.37\n",
      "2026-02-13T18:35:34.525402+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:34.526082+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:34.527190+0000 | compress_modules | INFO - Quantizing model.layers.22.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:35.744364+0000 | compress | METRIC - time 1.22s\n",
      "2026-02-13T18:35:35.746420+0000 | compress | METRIC - error 34.40\n",
      "2026-02-13T18:35:35.747556+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:35.748287+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(23/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 310.78it/s]\n",
      "(24/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 296.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:42.538091+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:43.743796+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:35:43.745765+0000 | compress | METRIC - error 792.33\n",
      "2026-02-13T18:35:43.746628+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:43.747154+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:35:43.748075+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:35:44.937935+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:44.939874+0000 | compress | METRIC - error 237.50\n",
      "2026-02-13T18:35:44.940669+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:44.941296+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:44.942238+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:35:46.124277+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:35:46.126251+0000 | compress | METRIC - error 300.40\n",
      "2026-02-13T18:35:46.127059+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:46.127735+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:46.128662+0000 | compress_modules | INFO - Quantizing model.layers.23.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:47.314365+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:47.316304+0000 | compress | METRIC - error 60.80\n",
      "2026-02-13T18:35:47.317068+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:47.317625+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(24/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 310.76it/s]\n",
      "(25/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 266.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:54.494890+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:35:55.697035+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:35:55.698933+0000 | compress | METRIC - error 1145.97\n",
      "2026-02-13T18:35:55.699796+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:55.700336+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:35:55.701438+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:35:56.931038+0000 | compress | METRIC - time 1.23s\n",
      "2026-02-13T18:35:56.933036+0000 | compress | METRIC - error 308.74\n",
      "2026-02-13T18:35:56.933964+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:56.934555+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:56.935630+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:35:58.125072+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:35:58.127040+0000 | compress | METRIC - error 373.43\n",
      "2026-02-13T18:35:58.127853+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:58.128380+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:35:58.129489+0000 | compress_modules | INFO - Quantizing model.layers.24.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:35:59.342609+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:35:59.344557+0000 | compress | METRIC - error 37.73\n",
      "2026-02-13T18:35:59.345367+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:35:59.346073+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(25/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 314.72it/s]\n",
      "(26/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 299.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:06.053283+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:07.256581+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:07.258612+0000 | compress | METRIC - error 1324.48\n",
      "2026-02-13T18:36:07.259416+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:07.260053+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:36:07.261349+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:36:08.483202+0000 | compress | METRIC - time 1.22s\n",
      "2026-02-13T18:36:08.485158+0000 | compress | METRIC - error 339.36\n",
      "2026-02-13T18:36:08.486166+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:08.486800+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:08.488079+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:36:09.685930+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:09.687877+0000 | compress | METRIC - error 542.17\n",
      "2026-02-13T18:36:09.688691+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:09.689175+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:09.690107+0000 | compress_modules | INFO - Quantizing model.layers.25.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:36:10.878326+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:10.880211+0000 | compress | METRIC - error 31.44\n",
      "2026-02-13T18:36:10.880899+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:10.881384+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(26/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.75it/s]\n",
      "(27/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 301.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:17.576053+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:18.795288+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:36:18.797202+0000 | compress | METRIC - error 1571.71\n",
      "2026-02-13T18:36:18.798012+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:18.798603+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:36:18.799825+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:36:19.994497+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:19.996483+0000 | compress | METRIC - error 431.58\n",
      "2026-02-13T18:36:19.997277+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:19.997897+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:19.999360+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:36:21.189925+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:21.191815+0000 | compress | METRIC - error 623.70\n",
      "2026-02-13T18:36:21.192686+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:21.193369+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:21.194401+0000 | compress_modules | INFO - Quantizing model.layers.26.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:36:22.398798+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:22.400754+0000 | compress | METRIC - error 26.35\n",
      "2026-02-13T18:36:22.401777+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:22.402324+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(27/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 309.70it/s]\n",
      "(28/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 300.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:29.158365+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:30.362250+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:30.364148+0000 | compress | METRIC - error 2363.99\n",
      "2026-02-13T18:36:30.365007+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:30.365564+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:36:30.366501+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:36:31.544925+0000 | compress | METRIC - time 1.18s\n",
      "2026-02-13T18:36:31.546064+0000 | compress | METRIC - error 617.77\n",
      "2026-02-13T18:36:31.546957+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:31.547575+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:31.548595+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:36:32.741996+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:32.743970+0000 | compress | METRIC - error 1132.52\n",
      "2026-02-13T18:36:32.744825+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:32.745427+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:32.746661+0000 | compress_modules | INFO - Quantizing model.layers.27.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:36:33.955423+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:36:33.957335+0000 | compress | METRIC - error 33.98\n",
      "2026-02-13T18:36:33.958177+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:33.958745+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(28/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 313.04it/s]\n",
      "(29/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 257.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:41.251525+0000 | compress_modules | INFO - Quantizing model.layers.28.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:42.455607+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:42.457553+0000 | compress | METRIC - error 2703.70\n",
      "2026-02-13T18:36:42.458297+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:42.458820+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:36:42.459725+0000 | compress_modules | INFO - Quantizing model.layers.28.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:36:43.652813+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:43.654854+0000 | compress | METRIC - error 704.62\n",
      "2026-02-13T18:36:43.656057+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:43.656825+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:43.658043+0000 | compress_modules | INFO - Quantizing model.layers.28.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:36:44.843911+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:44.845838+0000 | compress | METRIC - error 1428.20\n",
      "2026-02-13T18:36:44.846670+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:44.847314+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:44.848406+0000 | compress_modules | INFO - Quantizing model.layers.28.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:36:46.053926+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:46.055935+0000 | compress | METRIC - error 53.06\n",
      "2026-02-13T18:36:46.057055+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:46.057713+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(29/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 315.39it/s]\n",
      "(30/31): Calibrating: 100%|██████████| 1024/1024 [00:03<00:00, 297.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:52.784920+0000 | compress_modules | INFO - Quantizing model.layers.29.self_attn.q_proj using 1024 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:36:53.999366+0000 | compress | METRIC - time 1.21s\n",
      "2026-02-13T18:36:54.001504+0000 | compress | METRIC - error 2680.24\n",
      "2026-02-13T18:36:54.002671+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:54.003314+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n",
      "2026-02-13T18:36:54.004413+0000 | compress_modules | INFO - Quantizing model.layers.29.self_attn.k_proj using 1024 samples\n",
      "2026-02-13T18:36:55.199883+0000 | compress | METRIC - time 1.19s\n",
      "2026-02-13T18:36:55.201902+0000 | compress | METRIC - error 766.38\n",
      "2026-02-13T18:36:55.202997+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:55.203641+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:55.204630+0000 | compress_modules | INFO - Quantizing model.layers.29.self_attn.v_proj using 1024 samples\n",
      "2026-02-13T18:36:56.401971+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:56.403832+0000 | compress | METRIC - error 1155.19\n",
      "2026-02-13T18:36:56.404675+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:56.405219+0000 | compress | METRIC - Compressed module size: 2.121728 MB\n",
      "2026-02-13T18:36:56.406295+0000 | compress_modules | INFO - Quantizing model.layers.29.self_attn.o_proj using 1024 samples\n",
      "2026-02-13T18:36:57.605994+0000 | compress | METRIC - time 1.20s\n",
      "2026-02-13T18:36:57.608010+0000 | compress | METRIC - error 34.77\n",
      "2026-02-13T18:36:57.609100+0000 | compress | METRIC - GPU 0 | usage: 2.40% | total memory: 85 GB\n",
      "2026-02-13T18:36:57.609791+0000 | compress | METRIC - Compressed module size: 8.486912 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(30/31): Propagating: 100%|██████████| 1024/1024 [00:03<00:00, 306.96it/s]\n",
      "(31/31): Calibrating: 100%|██████████| 1024/1024 [00:00<00:00, 1049.23it/s]\n",
      "(31/31): Propagating: 100%|██████████| 1024/1024 [00:00<00:00, 1070.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:37:02.920003+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:37:02.962827+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "[INFO] GPTQ(attention-only) success (scheme=W4A16)\n",
      "[INFO] quantization complete (applied_mode=gptq_attn_only_w4a16, applied_scheme=W4A16)\n"
     ]
    }
   ],
   "source": [
    "def build_gptq_modifier(scheme_name):\n",
    "    return GPTQModifier(\n",
    "        scheme=scheme_name,\n",
    "        targets=ATTN_TARGETS,\n",
    "        ignore=IGNORE,\n",
    "    )\n",
    "\n",
    "\n",
    "candidate_schemes = [PRIMARY_SCHEME]\n",
    "if FALLBACK_SCHEME and FALLBACK_SCHEME not in candidate_schemes:\n",
    "    candidate_schemes.append(FALLBACK_SCHEME)\n",
    "\n",
    "applied_scheme = None\n",
    "applied_mode = None\n",
    "last_error = None\n",
    "\n",
    "for idx, scheme_name in enumerate(candidate_schemes, start=1):\n",
    "    print(\n",
    "        f\"[INFO] GPTQ(attention-only) start ({idx}/{len(candidate_schemes)}, \"\n",
    "        f\"scheme={scheme_name}, samples={NUM_CALIBRATION_SAMPLES}, max_len={MAX_SEQUENCE_LENGTH})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        recipe = [build_gptq_modifier(scheme_name)]\n",
    "        oneshot(\n",
    "            model=model,\n",
    "            dataset=ds,\n",
    "            recipe=recipe,\n",
    "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "        )\n",
    "        applied_scheme = scheme_name\n",
    "        applied_mode = f\"gptq_attn_only_{scheme_name.lower()}\"\n",
    "        print(f\"[INFO] GPTQ(attention-only) success (scheme={scheme_name})\")\n",
    "        break\n",
    "    except (ValueError, RuntimeError) as e:\n",
    "        last_error = e\n",
    "        print(f\"[WARN] GPTQ(attention-only) failed (scheme={scheme_name}): {type(e).__name__}: {e}\")\n",
    "\n",
    "if applied_scheme is None:\n",
    "    raise RuntimeError(\n",
    "        f\"All quantization schemes failed: {candidate_schemes}. Last error: {last_error}\"\n",
    "    )\n",
    "\n",
    "print(f\"[INFO] quantization complete (applied_mode={applied_mode}, applied_scheme={applied_scheme})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1770373825078,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "LcqYW7m_d668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-13T18:37:03.335362+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 120it [00:00, 160.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quant recipe saved: /content/drive/MyDrive/LGAimers/sq_w4a16_attn_calmix1024/quant_recipe.json\n",
      "[INFO] 모델 저장 완료: /content/drive/MyDrive/LGAimers/sq_w4a16_attn_calmix1024\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(OUT_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "quant_recipe = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"calibration_sources\": CALIB_DATASETS,\n",
    "    \"calibration_seed\": CALIBRATION_SEED,\n",
    "    \"calibration_benchmark_exclude\": sorted(CALIBRATION_BENCHMARK_EXCLUDE),\n",
    "    \"calibration_source_stats\": calibration_source_stats,\n",
    "    \"num_calibration_samples\": NUM_CALIBRATION_SAMPLES,\n",
    "    \"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "    \"primary_scheme\": PRIMARY_SCHEME,\n",
    "    \"fallback_scheme\": FALLBACK_SCHEME,\n",
    "    \"applied_scheme\": applied_scheme,\n",
    "    \"applied_mode\": applied_mode,\n",
    "    \"targets\": ATTN_TARGETS,\n",
    "    \"ignore\": IGNORE,\n",
    "    \"out_dir\": OUT_DIR,\n",
    "}\n",
    "\n",
    "recipe_path = Path(OUT_DIR) / \"quant_recipe.json\"\n",
    "recipe_path.write_text(json.dumps(quant_recipe, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[INFO] quant recipe saved: {recipe_path}\")\n",
    "print(f\"[INFO] 모델 저장 완료: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 192028,
     "status": "aborted",
     "timestamp": 1770373825094,
     "user": {
      "displayName": "­최지훈 / 학생 / 전기·정보공학부",
      "userId": "08585621327904699455"
     },
     "user_tz": -540
    },
    "id": "nGoLxbTid-4j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 생성 완료: /content/drive/MyDrive/LGAimers/submit/sq_w4a16_attn_calmix1024.zip\n"
     ]
    }
   ],
   "source": [
    "zip_name = f\"/content/drive/MyDrive/LGAimers/submit/{Path(OUT_DIR).name}\"\n",
    "zip_path = Path(zip_name)\n",
    "\n",
    "zip_path.parent.mkdir(parents=True, exist_ok=True)  # 경로 없으면 생성\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    tmp_root = Path(tmpdir)\n",
    "    model_dir = tmp_root / \"model\"\n",
    "    shutil.copytree(OUT_DIR, model_dir)\n",
    "    shutil.make_archive(str(zip_path), \"zip\", root_dir=tmp_root, base_dir=\"model\")\n",
    "\n",
    "print(f\"[INFO] 생성 완료: {zip_name}.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPqB7R4DR3r6NQJMxe8lrmw",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00cce46a3062432f8f0ef1e0cb613c39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0254b1cfd4a14ac88739d68f6b065817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0855c5ccf25d43ac9dd33c89425b6dde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c41977b3e84448fa63c86867d02dd51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cfdcef81fcb4e88929e69e4e7fa23d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "133a1ff3125f40c09a076054a4f19d5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b21e94a4d6f4bf38bd1cc184336a4fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ce6af7aa75b4d0194d8f36a480517e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1eda342391f649d483703d939f3ade3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ce6af7aa75b4d0194d8f36a480517e5",
      "max": 256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cfdcef81fcb4e88929e69e4e7fa23d8",
      "value": 256
     }
    },
    "22d30c6d3cd547c599be3c177bb0ddc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_572bbd11ea5744588b657b3bce78afb5",
       "IPY_MODEL_1eda342391f649d483703d939f3ade3d",
       "IPY_MODEL_93fb5465bcf145949735038460467d2f"
      ],
      "layout": "IPY_MODEL_99a42dc7ecb24e88970ddc96970e388e"
     }
    },
    "2c0fd9d15c974a468719f159bc6b48b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2da239963de44ed8b4b80ef5cefdc963": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3142147fb68e46a29cfc7c116bb2cc90",
      "placeholder": "​",
      "style": "IPY_MODEL_61b99d17ff694de5be15ace4032607f5",
      "value": " 2.88k/? [00:00&lt;00:00, 223kB/s]"
     }
    },
    "3020f237b81f4e1c826641043303009a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_638af45d596f44b4924ab8bc7640b5cf",
      "placeholder": "​",
      "style": "IPY_MODEL_7691ddbb6258474ba3b413e8cc77e067",
      "value": "README.md: "
     }
    },
    "3142147fb68e46a29cfc7c116bb2cc90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35912d36448a488cb576eeb5f2e043b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "374b9cb109f64f04ad77df5e0d9b6acc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90562e41607d44309ea17e4840a6f091",
      "placeholder": "​",
      "style": "IPY_MODEL_0c41977b3e84448fa63c86867d02dd51",
      "value": "data/train.parquet: 100%"
     }
    },
    "432ee61f2d0c49cb8c6a9ec540757d6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4567c4dac35948ce97fcb3b556e44b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4dbb9d4a86284fe8b680f240de006a79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56e74919f40d475fb7f313484e1b3b90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_133a1ff3125f40c09a076054a4f19d5f",
      "placeholder": "​",
      "style": "IPY_MODEL_ef21b4f46d6947198ad5511460712326",
      "value": "Generating train split: 100%"
     }
    },
    "572bbd11ea5744588b657b3bce78afb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_432ee61f2d0c49cb8c6a9ec540757d6e",
      "placeholder": "​",
      "style": "IPY_MODEL_4567c4dac35948ce97fcb3b556e44b9a",
      "value": "Map: 100%"
     }
    },
    "617a008fc86e427285d30004a9887ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61b99d17ff694de5be15ace4032607f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "638af45d596f44b4924ab8bc7640b5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6799fa726f7040bb8798a7cacdae656b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56e74919f40d475fb7f313484e1b3b90",
       "IPY_MODEL_a9322ea1cf7f482ca1915fb8f117db6b",
       "IPY_MODEL_e56a5099024b428096888d69458fcf71"
      ],
      "layout": "IPY_MODEL_c24d532dd9214848841889eb577f7e79"
     }
    },
    "716daef2903a4b26b94f0dacd93ec439": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6614d2c5dd248038efacb895268f37b",
      "placeholder": "​",
      "style": "IPY_MODEL_bcc86fd3404f42eab24f1b3b84d0a84d",
      "value": "Tokenizing: 100%"
     }
    },
    "7691ddbb6258474ba3b413e8cc77e067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79b5c92fd18b413f8a05ef48b19ea1c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_955b76360c58473a835cf96e2f1b53a8",
      "placeholder": "​",
      "style": "IPY_MODEL_e55f82a8245d4b359225d7dde42ef682",
      "value": " 256/256 [00:00&lt;00:00, 336.92 examples/s]"
     }
    },
    "7a83da2058af4b1ba16cbc1642a9d870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_716daef2903a4b26b94f0dacd93ec439",
       "IPY_MODEL_b6b1c94a9abb4a8aa4b328ec027f9b3b",
       "IPY_MODEL_79b5c92fd18b413f8a05ef48b19ea1c3"
      ],
      "layout": "IPY_MODEL_35912d36448a488cb576eeb5f2e043b6"
     }
    },
    "8158348ca062439e8340097dcd662ffa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a0baf40b6534763a2e2c2c101efa1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d7aec96dd4a4dc2b1000e87b3b48eb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90562e41607d44309ea17e4840a6f091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93fb5465bcf145949735038460467d2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0855c5ccf25d43ac9dd33c89425b6dde",
      "placeholder": "​",
      "style": "IPY_MODEL_8a0baf40b6534763a2e2c2c101efa1fe",
      "value": " 256/256 [00:00&lt;00:00, 2222.91 examples/s]"
     }
    },
    "955b76360c58473a835cf96e2f1b53a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99a42dc7ecb24e88970ddc96970e388e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f967f2336724d5bb8fa304acfc0172b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c221d9ea204f44e489f7d2b1103e1f64",
      "max": 1938739123,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f08a8bcc4ad7495380a16a75ed2a821a",
      "value": 1938739123
     }
    },
    "a06c20cad5a741e385844d0a9efddb05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_374b9cb109f64f04ad77df5e0d9b6acc",
       "IPY_MODEL_9f967f2336724d5bb8fa304acfc0172b",
       "IPY_MODEL_dfc9b159922642a8a20a8527d02b6c63"
      ],
      "layout": "IPY_MODEL_bbc9a0f5def74f178f8351cfb14a4184"
     }
    },
    "a6614d2c5dd248038efacb895268f37b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a81140f334f941f18aa669efe7c3482f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec8115a116764bc898b1050d1b206f1b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8158348ca062439e8340097dcd662ffa",
      "value": 1
     }
    },
    "a9322ea1cf7f482ca1915fb8f117db6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f001cf3b5e634d46b9c7770a52355468",
      "max": 1000000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0254b1cfd4a14ac88739d68f6b065817",
      "value": 1000000
     }
    },
    "b299e23224d24d3eb776cfcf092af1c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3020f237b81f4e1c826641043303009a",
       "IPY_MODEL_a81140f334f941f18aa669efe7c3482f",
       "IPY_MODEL_2da239963de44ed8b4b80ef5cefdc963"
      ],
      "layout": "IPY_MODEL_1b21e94a4d6f4bf38bd1cc184336a4fc"
     }
    },
    "b6b1c94a9abb4a8aa4b328ec027f9b3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_617a008fc86e427285d30004a9887ca1",
      "max": 256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4dbb9d4a86284fe8b680f240de006a79",
      "value": 256
     }
    },
    "bbc9a0f5def74f178f8351cfb14a4184": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcc86fd3404f42eab24f1b3b84d0a84d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c221d9ea204f44e489f7d2b1103e1f64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c24d532dd9214848841889eb577f7e79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccd58b180caa45b287381db67ed859f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfc9b159922642a8a20a8527d02b6c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccd58b180caa45b287381db67ed859f2",
      "placeholder": "​",
      "style": "IPY_MODEL_00cce46a3062432f8f0ef1e0cb613c39",
      "value": " 1.94G/1.94G [00:12&lt;00:00, 229MB/s]"
     }
    },
    "e55f82a8245d4b359225d7dde42ef682": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e56a5099024b428096888d69458fcf71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c0fd9d15c974a468719f159bc6b48b2",
      "placeholder": "​",
      "style": "IPY_MODEL_8d7aec96dd4a4dc2b1000e87b3b48eb9",
      "value": " 1000000/1000000 [00:27&lt;00:00, 47337.98 examples/s]"
     }
    },
    "ec8115a116764bc898b1050d1b206f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "ef21b4f46d6947198ad5511460712326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f001cf3b5e634d46b9c7770a52355468": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f08a8bcc4ad7495380a16a75ed2a821a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
