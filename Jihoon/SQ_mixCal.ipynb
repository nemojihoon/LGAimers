{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llmcompressor\n",
    "%pip install -U llmcompressor transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/content/drive/MyDrive/LGAimers/base_model\"\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 1024\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "CALIBRATION_SEED = 42\n",
    "\n",
    "CALIB_DATASETS = [\n",
    "    {\n",
    "        \"id\": \"LGAI-EXAONE/MANTA-1M\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 512,\n",
    "        \"format\": \"manta\",\n",
    "        \"priority\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"nlpai-lab/kullm-v2\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 256,\n",
    "        \"format\": \"instruction\",\n",
    "        \"priority\": 2,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"heegyu/OIG-small-chip2-ko\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 192,\n",
    "        \"format\": \"oig_human_bot\",\n",
    "        \"priority\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"beomi/KoAlpaca-v1.1a\",\n",
    "        \"split\": \"train\",\n",
    "        \"n_samples\": 64,\n",
    "        \"format\": \"instruction\",\n",
    "        \"priority\": 4,\n",
    "    },\n",
    "]\n",
    "\n",
    "CALIBRATION_BENCHMARK_EXCLUDE = {\n",
    "    \"LGAI-EXAONE/KoMT-Bench\",\n",
    "    \"LGAI-EXAONE/KMMLU-Redux\",\n",
    "    \"LGAI-EXAONE/KMMLU-Pro\",\n",
    "}\n",
    "KOALPACA_FALLBACK_ID = \"nlpai-lab/kullm-v2\"\n",
    "\n",
    "SMOOTHING_STRENGTH = 0.40\n",
    "QUANT_SCHEME = \"W8A8\"\n",
    "QUANT_SCOPE = \"mlp_only\"\n",
    "MLP_TARGETS = [\n",
    "    \"re:.*mlp\\.gate_proj\",\n",
    "    \"re:.*mlp\\.up_proj\",\n",
    "    \"re:.*mlp\\.down_proj\",\n",
    "]\n",
    "IGNORE = [\"embed_tokens\", \"lm_head\"]\n",
    "\n",
    "assert sum(spec[\"n_samples\"] for spec in CALIB_DATASETS) == NUM_CALIBRATION_SAMPLES, (\n",
    "    \"CALIB_DATASETS 샘플 합계와 NUM_CALIBRATION_SAMPLES가 일치해야 합니다.\"\n",
    ")\n",
    "assert not any(spec[\"id\"] in CALIBRATION_BENCHMARK_EXCLUDE for spec in CALIB_DATASETS), (\n",
    "    \"평가용 벤치마크 데이터셋은 calibration에서 제외해야 합니다.\"\n",
    ")\n",
    "\n",
    "if QUANT_SCOPE != \"mlp_only\":\n",
    "    raise ValueError(f\"지원하지 않는 QUANT_SCOPE: {QUANT_SCOPE}\")\n",
    "\n",
    "QUANT_TARGETS = MLP_TARGETS\n",
    "OUT_DIR = \"/content/drive/MyDrive/LGAimers/sq_mixcal_w8a8_mlp_s040_calmix1024\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] 모델 로드 중...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"[INFO] 모델/토크나이저 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_target_modules(model, target_patterns):\n",
    "    all_module_names = [name for name, _ in model.named_modules()]\n",
    "\n",
    "    matched_by_pattern = {}\n",
    "    for pattern in target_patterns:\n",
    "        regex = pattern[len(\"re:\"):] if pattern.startswith(\"re:\") else re.escape(pattern)\n",
    "        matches = [name for name in all_module_names if re.fullmatch(regex, name)]\n",
    "        matched_by_pattern[pattern] = matches\n",
    "        print(f\"[TARGET] pattern={pattern} matches={len(matches)} sample={matches[:3]}\")\n",
    "\n",
    "    unmatched_patterns = [p for p, m in matched_by_pattern.items() if not m]\n",
    "    if unmatched_patterns:\n",
    "        raise AssertionError(f\"No module matched for patterns: {unmatched_patterns}\")\n",
    "\n",
    "    unique_matches = sorted({m for matches in matched_by_pattern.values() for m in matches})\n",
    "\n",
    "    expected_suffixes = {\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    }\n",
    "    per_layer = {}\n",
    "    for name in unique_matches:\n",
    "        parts = name.split(\".\")\n",
    "        if len(parts) < 5 or parts[0] != \"model\" or parts[1] != \"layers\" or not parts[2].isdigit():\n",
    "            continue\n",
    "        layer_id = int(parts[2])\n",
    "        suffix = \".\".join(parts[3:])\n",
    "        per_layer.setdefault(layer_id, set()).add(suffix)\n",
    "\n",
    "    missing_per_layer = {\n",
    "        layer_id: sorted(expected_suffixes - suffixes)\n",
    "        for layer_id, suffixes in per_layer.items()\n",
    "        if suffixes != expected_suffixes\n",
    "    }\n",
    "    if missing_per_layer:\n",
    "        raise AssertionError(f\"Incomplete MLP targets per layer: {missing_per_layer}\")\n",
    "\n",
    "    layer_ids = sorted(per_layer)\n",
    "    print(f\"[TARGET] total_unique_matches={len(unique_matches)}\")\n",
    "    if layer_ids:\n",
    "        print(f\"[TARGET] layer_count={len(layer_ids)}, layer_range=({layer_ids[0]}, {layer_ids[-1]})\")\n",
    "\n",
    "    return unique_matches\n",
    "\n",
    "\n",
    "QUANT_TARGET_MATCHED_MODULES = collect_target_modules(model, QUANT_TARGETS)\n",
    "print(\"[INFO] MLP-only target validation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc89a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_modules(model):\n",
    "    tracked_keys = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"input_layernorm\",\n",
    "        \"post_attention_layernorm\",\n",
    "    ]\n",
    "    mod_index = {key: [] for key in tracked_keys}\n",
    "\n",
    "    for name, _ in model.named_modules():\n",
    "        for key in tracked_keys:\n",
    "            if name.endswith(key):\n",
    "                mod_index[key].append(name)\n",
    "\n",
    "    return mod_index\n",
    "\n",
    "\n",
    "def build_exaone_sq_mappings(mod_index):\n",
    "    module_counts = {k: len(v) for k, v in mod_index.items()}\n",
    "    module_samples = {k: v[:5] for k, v in mod_index.items()}\n",
    "\n",
    "    has_input_ln = module_counts[\"input_layernorm\"] > 0\n",
    "    has_post_attn_ln = module_counts[\"post_attention_layernorm\"] > 0\n",
    "    has_gate = module_counts[\"gate_proj\"] > 0\n",
    "    has_up = module_counts[\"up_proj\"] > 0\n",
    "\n",
    "    mappings = []\n",
    "    mode = \"disabled\"\n",
    "\n",
    "    if has_post_attn_ln and has_gate and has_up:\n",
    "        mappings = [\n",
    "            [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"]\n",
    "        ]\n",
    "        mode = \"mlp_only\"\n",
    "\n",
    "    reason_parts = []\n",
    "    if not has_input_ln:\n",
    "        reason_parts.append(\"input_layernorm not found -> qkv smoothing disabled\")\n",
    "    if not has_post_attn_ln:\n",
    "        reason_parts.append(\"post_attention_layernorm not found\")\n",
    "    if not has_gate or not has_up:\n",
    "        reason_parts.append(\"gate_proj/up_proj pair incomplete\")\n",
    "    if not reason_parts:\n",
    "        reason_parts.append(\"EXAONE policy: qkv smoothing disabled, mlp-only smoothing enabled\")\n",
    "\n",
    "    diag = {\n",
    "        \"module_counts\": module_counts,\n",
    "        \"module_samples\": module_samples,\n",
    "        \"reason\": \"; \".join(reason_parts),\n",
    "    }\n",
    "\n",
    "    return mappings, mode, diag\n",
    "\n",
    "\n",
    "def normalize_sq_mappings(mappings):\n",
    "    normalized = []\n",
    "    for m in mappings:\n",
    "        if isinstance(m, dict):\n",
    "            normalized.append([m[\"balance_layers\"], m[\"smooth_layers\"]])\n",
    "        else:\n",
    "            normalized.append(m)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "module_index = collect_modules(model)\n",
    "SQ_MAPPINGS, SQ_MODE, SQ_DIAG = build_exaone_sq_mappings(module_index)\n",
    "SQ_MAPPINGS = normalize_sq_mappings(SQ_MAPPINGS)\n",
    "SQ_ENABLED = len(SQ_MAPPINGS) > 0\n",
    "\n",
    "print(\"[SQ] EXAONE mapping diagnostics\")\n",
    "for key in [\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"input_layernorm\", \"post_attention_layernorm\"]:\n",
    "    print(f\"[SQ] {key}: count={SQ_DIAG['module_counts'][key]}, sample={SQ_DIAG['module_samples'][key]}\")\n",
    "\n",
    "print(f\"[SQ] mode={SQ_MODE}, enabled={SQ_ENABLED}, mappings={len(SQ_MAPPINGS)}\")\n",
    "print(f\"[SQ] reason={SQ_DIAG['reason']}\")\n",
    "if SQ_ENABLED:\n",
    "    print(f\"[SQ] mappings={SQ_MAPPINGS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a14fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] 캘리브레이션 데이터 로드 중...\")\n",
    "\n",
    "def _clean_text(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    return str(value).strip()\n",
    "\n",
    "def _normalize_manta_conversations(conversations):\n",
    "    if not isinstance(conversations, list):\n",
    "        return None\n",
    "\n",
    "    normalized = []\n",
    "    for turn in conversations:\n",
    "        if not isinstance(turn, dict):\n",
    "            continue\n",
    "        role = _clean_text(turn.get(\"role\")).lower()\n",
    "        content = _clean_text(turn.get(\"content\"))\n",
    "        if role in {\"system\", \"user\", \"assistant\"} and content:\n",
    "            normalized.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    has_user = any(turn[\"role\"] == \"user\" for turn in normalized)\n",
    "    has_assistant = any(turn[\"role\"] == \"assistant\" for turn in normalized)\n",
    "    if not (has_user and has_assistant):\n",
    "        return None\n",
    "    return normalized\n",
    "\n",
    "def _normalize_instruction_turn(example):\n",
    "    instruction = _clean_text(example.get(\"instruction\"))\n",
    "    input_text = _clean_text(example.get(\"input\"))\n",
    "    output_text = _clean_text(example.get(\"output\"))\n",
    "\n",
    "    user_content = instruction\n",
    "    if input_text:\n",
    "        user_content = f\"{instruction}\\n\\n{input_text}\" if instruction else input_text\n",
    "\n",
    "    if not user_content or not output_text:\n",
    "        return None\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": output_text},\n",
    "    ]\n",
    "\n",
    "def _normalize_oig_pair(example):\n",
    "    user_content = _clean_text(example.get(\"user_translated\")) or _clean_text(example.get(\"user\"))\n",
    "    assistant_content = _clean_text(example.get(\"chip2_translated\")) or _clean_text(example.get(\"chip2\"))\n",
    "    if user_content and assistant_content:\n",
    "        return [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "        ]\n",
    "\n",
    "    raw = _clean_text(example.get(\"text\"))\n",
    "    if not raw:\n",
    "        return None\n",
    "\n",
    "    pattern = re.compile(r\"<human>\\s*(.*?)\\s*<bot>\\s*(.*?)(?=<human>|$)\", re.IGNORECASE | re.DOTALL)\n",
    "    match = pattern.search(raw)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    user_content = _clean_text(match.group(1))\n",
    "    assistant_content = _clean_text(match.group(2))\n",
    "    if not user_content or not assistant_content:\n",
    "        return None\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "    ]\n",
    "\n",
    "def normalize_example(example, source_format):\n",
    "    if source_format == \"manta\":\n",
    "        conversations = _normalize_manta_conversations(example.get(\"conversations\"))\n",
    "    elif source_format == \"instruction\":\n",
    "        conversations = _normalize_instruction_turn(example)\n",
    "    elif source_format == \"oig_human_bot\":\n",
    "        conversations = _normalize_oig_pair(example)\n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 source format: {source_format}\")\n",
    "\n",
    "    if conversations is None:\n",
    "        return None\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "def _conversation_hash(record):\n",
    "    payload = json.dumps(record[\"conversations\"], ensure_ascii=False, sort_keys=True)\n",
    "    return hashlib.sha1(payload.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _load_single_spec(spec, seed, global_seen_hashes):\n",
    "    dataset_id = spec[\"id\"]\n",
    "    split = spec[\"split\"]\n",
    "    target_n = spec[\"n_samples\"]\n",
    "    source_format = spec[\"format\"]\n",
    "\n",
    "    raw_ds = load_dataset(dataset_id, split=split).shuffle(seed=seed)\n",
    "    dataset_columns = list(raw_ds.features.keys())\n",
    "\n",
    "    selected = []\n",
    "    scanned = 0\n",
    "    dropped = 0\n",
    "    duplicate = 0\n",
    "\n",
    "    for example in raw_ds:\n",
    "        scanned += 1\n",
    "        normalized = normalize_example(example, source_format)\n",
    "        if normalized is None:\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        conv_hash = _conversation_hash(normalized)\n",
    "        if conv_hash in global_seen_hashes:\n",
    "            duplicate += 1\n",
    "            continue\n",
    "\n",
    "        global_seen_hashes.add(conv_hash)\n",
    "        selected.append(normalized)\n",
    "        if len(selected) >= target_n:\n",
    "            break\n",
    "\n",
    "    if len(selected) < target_n:\n",
    "        raise RuntimeError(\n",
    "            f\"{dataset_id}에서 목표 샘플({target_n})을 채우지 못했습니다. \"\n",
    "            f\"selected={len(selected)}, scanned={scanned}, dropped={dropped}, duplicate={duplicate}, \"\n",
    "            f\"columns={dataset_columns}\"\n",
    "        )\n",
    "\n",
    "    normalized_ds = Dataset.from_list(selected)\n",
    "    stats = {\n",
    "        \"id\": dataset_id,\n",
    "        \"split\": split,\n",
    "        \"format\": source_format,\n",
    "        \"target_n\": target_n,\n",
    "        \"selected_n\": len(selected),\n",
    "        \"scanned_n\": scanned,\n",
    "        \"dropped_n\": dropped,\n",
    "        \"duplicate_n\": duplicate,\n",
    "        \"columns\": dataset_columns,\n",
    "    }\n",
    "    return normalized_ds, stats\n",
    "\n",
    "def build_calibration_dataset(specs, seed):\n",
    "    if not specs:\n",
    "        raise ValueError(\"CALIB_DATASETS가 비어 있습니다.\")\n",
    "\n",
    "    specs_sorted = sorted(specs, key=lambda x: x[\"priority\"])\n",
    "    subset_list = []\n",
    "    stats_list = []\n",
    "    global_seen_hashes = set()\n",
    "\n",
    "    for spec in specs_sorted:\n",
    "        try:\n",
    "            subset, stats = _load_single_spec(spec, seed=seed, global_seen_hashes=global_seen_hashes)\n",
    "            subset_list.append(subset)\n",
    "            stats_list.append(stats)\n",
    "        except Exception as exc:\n",
    "            if spec[\"id\"] == \"beomi/KoAlpaca-v1.1a\":\n",
    "                print(f\"[WARN] {spec['id']} 로드 실패 -> {KOALPACA_FALLBACK_ID}로 대체: {type(exc).__name__}: {exc}\")\n",
    "                fallback_spec = dict(spec)\n",
    "                fallback_spec[\"id\"] = KOALPACA_FALLBACK_ID\n",
    "                fallback_spec[\"split\"] = \"train\"\n",
    "                fallback_spec[\"format\"] = \"instruction\"\n",
    "                subset, stats = _load_single_spec(fallback_spec, seed=seed, global_seen_hashes=global_seen_hashes)\n",
    "                stats[\"fallback_from\"] = spec[\"id\"]\n",
    "                subset_list.append(subset)\n",
    "                stats_list.append(stats)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "    mixed = concatenate_datasets(subset_list).shuffle(seed=seed)\n",
    "\n",
    "    if len(mixed) != NUM_CALIBRATION_SAMPLES:\n",
    "        raise RuntimeError(\n",
    "            f\"최종 calibration 샘플 수가 다릅니다: expected={NUM_CALIBRATION_SAMPLES}, actual={len(mixed)}\"\n",
    "        )\n",
    "\n",
    "    for stats in stats_list:\n",
    "        if stats[\"selected_n\"] < min(5, stats[\"target_n\"]):\n",
    "            raise RuntimeError(f\"정규화 성공 샘플이 부족합니다: {stats}\")\n",
    "\n",
    "    return mixed, stats_list\n",
    "\n",
    "def _contains_korean(text):\n",
    "    return any(\"가\" <= ch <= \"힣\" for ch in text)\n",
    "\n",
    "def _compute_ko_char_ratio(records):\n",
    "    total_chars = 0\n",
    "    ko_chars = 0\n",
    "    for record in records:\n",
    "        for turn in record[\"conversations\"]:\n",
    "            content = turn[\"content\"]\n",
    "            total_chars += len(content)\n",
    "            ko_chars += sum(1 for ch in content if \"가\" <= ch <= \"힣\")\n",
    "    if total_chars == 0:\n",
    "        return 0.0\n",
    "    return ko_chars / total_chars\n",
    "\n",
    "ds, calibration_source_stats = build_calibration_dataset(CALIB_DATASETS, seed=CALIBRATION_SEED)\n",
    "\n",
    "records = list(ds)\n",
    "ko_char_ratio = _compute_ko_char_ratio(records)\n",
    "ko_record_ratio = (\n",
    "    sum(1 for item in records if _contains_korean(json.dumps(item[\"conversations\"], ensure_ascii=False)))\n",
    "    / max(len(records), 1)\n",
    ")\n",
    "\n",
    "print(f\"[INFO] 혼합 캘리브레이션 샘플 수: {len(records)}\")\n",
    "print(f\"[INFO] 한글 문자 비율: {ko_char_ratio:.4f}\")\n",
    "print(f\"[INFO] 한글 포함 샘플 비율: {ko_record_ratio:.4f}\")\n",
    "print(\"[INFO] 소스별 통계:\")\n",
    "for stats in calibration_source_stats:\n",
    "    print(f\"  - {stats}\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"],\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "print(\"[INFO] 데이터 전처리 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w8a8_modifier():\n",
    "    quant_builder_attempts = []\n",
    "\n",
    "    quant_candidates = [\n",
    "        {\n",
    "            \"scheme\": QUANT_SCHEME,\n",
    "            \"targets\": QUANT_TARGETS,\n",
    "            \"ignore\": IGNORE,\n",
    "        },\n",
    "        {\n",
    "            \"scheme\": {QUANT_SCHEME: QUANT_TARGETS},\n",
    "            \"ignore\": IGNORE,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for idx, kwargs in enumerate(quant_candidates, start=1):\n",
    "        try:\n",
    "            modifier = QuantizationModifier(**kwargs)\n",
    "            return modifier, {\n",
    "                \"builder\": \"QuantizationModifier\",\n",
    "                \"attempts\": quant_builder_attempts,\n",
    "                \"selected_kwargs\": kwargs,\n",
    "            }\n",
    "        except Exception as exc:\n",
    "            quant_builder_attempts.append(\n",
    "                {\n",
    "                    \"candidate\": idx,\n",
    "                    \"kwargs\": kwargs,\n",
    "                    \"error\": f\"{type(exc).__name__}: {exc}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    print(\"[WARN] QuantizationModifier 생성 실패 -> GPTQModifier(scheme=W8A8)로 폴백합니다.\")\n",
    "    modifier = GPTQModifier(\n",
    "        scheme=QUANT_SCHEME,\n",
    "        targets=QUANT_TARGETS,\n",
    "        ignore=IGNORE,\n",
    "    )\n",
    "    return modifier, {\n",
    "        \"builder\": \"GPTQModifier_fallback\",\n",
    "        \"attempts\": quant_builder_attempts,\n",
    "        \"selected_kwargs\": {\n",
    "            \"scheme\": QUANT_SCHEME,\n",
    "            \"targets\": QUANT_TARGETS,\n",
    "            \"ignore\": IGNORE,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"[INFO] SmoothQuant + {QUANT_SCHEME} 시작 (scope={QUANT_SCOPE}, strength={SMOOTHING_STRENGTH}, \"\n",
    "    f\"samples={NUM_CALIBRATION_SAMPLES}, max_len={MAX_SEQUENCE_LENGTH}, sq_mode={SQ_MODE})\"\n",
    ")\n",
    "\n",
    "sq_mappings = normalize_sq_mappings(SQ_MAPPINGS)\n",
    "if SQ_ENABLED:\n",
    "    print(f\"[SQ] normalized mappings={sq_mappings}\")\n",
    "\n",
    "applied_mode = \"w8a8_only_fallback\"\n",
    "last_error = None\n",
    "quant_builder_diag = None\n",
    "quant_modifier_impl = None\n",
    "\n",
    "try:\n",
    "    quant_modifier, quant_builder_diag = build_w8a8_modifier()\n",
    "    quant_modifier_impl = type(quant_modifier).__name__\n",
    "\n",
    "    if SQ_ENABLED:\n",
    "        recipe = [\n",
    "            SmoothQuantModifier(\n",
    "                smoothing_strength=SMOOTHING_STRENGTH,\n",
    "                mappings=sq_mappings,\n",
    "            ),\n",
    "            quant_modifier,\n",
    "        ]\n",
    "        oneshot(\n",
    "            model=model,\n",
    "            dataset=ds,\n",
    "            recipe=recipe,\n",
    "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "        )\n",
    "        applied_mode = f\"smoothquant_{SQ_MODE}+w8a8\"\n",
    "    else:\n",
    "        print(\"[WARN] SmoothQuant 비활성 상태라 W8A8-only로 실행합니다.\")\n",
    "        print(f\"[WARN] reason: {SQ_DIAG['reason']}\")\n",
    "        recipe = [quant_modifier]\n",
    "        oneshot(\n",
    "            model=model,\n",
    "            dataset=ds,\n",
    "            recipe=recipe,\n",
    "            max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "            num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "        )\n",
    "except (ValueError, RuntimeError, TypeError) as exc:\n",
    "    last_error = exc\n",
    "    print(f\"[WARN] SmoothQuant 경로 실패: {type(exc).__name__}: {exc}\")\n",
    "    print(\"[WARN] W8A8-only 폴백으로 재실행합니다.\")\n",
    "\n",
    "    quant_modifier, quant_builder_diag = build_w8a8_modifier()\n",
    "    quant_modifier_impl = type(quant_modifier).__name__\n",
    "    recipe = [quant_modifier]\n",
    "    oneshot(\n",
    "        model=model,\n",
    "        dataset=ds,\n",
    "        recipe=recipe,\n",
    "        max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "        num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    )\n",
    "    applied_mode = \"w8a8_only_fallback\"\n",
    "\n",
    "print(\n",
    "    f\"[INFO] quantization complete (applied_mode={applied_mode}, \"\n",
    "    f\"scheme={QUANT_SCHEME}, scope={QUANT_SCOPE}, modifier={quant_modifier_impl})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe4e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(OUT_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "quant_recipe = {\n",
    "    \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"calibration_sources\": CALIB_DATASETS,\n",
    "    \"calibration_seed\": CALIBRATION_SEED,\n",
    "    \"calibration_benchmark_exclude\": sorted(CALIBRATION_BENCHMARK_EXCLUDE),\n",
    "    \"calibration_source_stats\": calibration_source_stats,\n",
    "    \"num_calibration_samples\": NUM_CALIBRATION_SAMPLES,\n",
    "    \"max_sequence_length\": MAX_SEQUENCE_LENGTH,\n",
    "    \"quant_scheme\": QUANT_SCHEME,\n",
    "    \"quant_scope\": QUANT_SCOPE,\n",
    "    \"quant_targets\": QUANT_TARGETS,\n",
    "    \"ignore\": IGNORE,\n",
    "    \"smoothing_strength\": SMOOTHING_STRENGTH,\n",
    "    \"sq_mode\": SQ_MODE,\n",
    "    \"sq_enabled\": SQ_ENABLED,\n",
    "    \"sq_mappings\": SQ_MAPPINGS,\n",
    "    \"sq_diag\": SQ_DIAG,\n",
    "    \"applied_mode\": applied_mode,\n",
    "    \"quant_modifier_impl\": quant_modifier_impl,\n",
    "    \"quant_builder_diag\": quant_builder_diag,\n",
    "    \"last_error\": str(last_error) if last_error is not None else None,\n",
    "    \"out_dir\": OUT_DIR,\n",
    "}\n",
    "\n",
    "recipe_path = Path(OUT_DIR) / \"quant_recipe.json\"\n",
    "recipe_path.write_text(json.dumps(quant_recipe, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[INFO] quant recipe saved: {recipe_path}\")\n",
    "print(f\"[INFO] 모델 저장 완료: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198af594",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = f\"/content/drive/MyDrive/LGAimers/submit/{Path(OUT_DIR).name}\"\n",
    "zip_path = Path(zip_name)\n",
    "\n",
    "zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    tmp_root = Path(tmpdir)\n",
    "    model_dir = tmp_root / \"model\"\n",
    "    shutil.copytree(OUT_DIR, model_dir)\n",
    "    shutil.make_archive(str(zip_path), \"zip\", root_dir=tmp_root, base_dir=\"model\")\n",
    "\n",
    "print(f\"[INFO] 생성 완료: {zip_name}.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(OUT_DIR)\n",
    "zip_file = Path(f\"{zip_name}.zip\")\n",
    "\n",
    "required_files = [\n",
    "    out_dir / \"config.json\",\n",
    "    out_dir / \"quant_recipe.json\",\n",
    "]\n",
    "\n",
    "missing = [str(p) for p in required_files if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"필수 파일 누락: {missing}\")\n",
    "\n",
    "has_tokenizer_artifact = any(\n",
    "    (out_dir / name).exists()\n",
    "    for name in [\"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\"]\n",
    ")\n",
    "if not has_tokenizer_artifact:\n",
    "    raise FileNotFoundError(\"토크나이저 산출물이 누락되었습니다.\")\n",
    "\n",
    "if not zip_file.exists():\n",
    "    raise FileNotFoundError(f\"zip 파일 누락: {zip_file}\")\n",
    "\n",
    "print(\"[VERIFY] 산출물 검증 성공\")\n",
    "print(f\"[VERIFY] out_dir={out_dir}\")\n",
    "print(f\"[VERIFY] zip={zip_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f212b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test 가이드 (실행 전 수동 적용)\n",
    "# 1) NUM_CALIBRATION_SAMPLES=64\n",
    "# 2) CALIB_DATASETS의 n_samples 합계를 64로 맞춤\n",
    "# 3) 전체 셀 실행 후 applied_mode / 산출물 검증 로그 확인\n",
    "print(\"[INFO] smoke test 가이드 셀입니다. 필요 시 설정값을 줄여 별도 실행하세요.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
